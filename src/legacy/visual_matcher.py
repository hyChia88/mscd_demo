import torch
from transformers import CLIPProcessor, CLIPModel
import numpy as np

class VisualAligner:
    def __init__(self):
        print("ğŸ‘ï¸ [VisualAligner] Initializing CLIP Model (Multimodal Embedding Space)...")
        # ä½¿ç”¨è¾ƒå°çš„ CLIP æ¨¡å‹ä»¥ä¾¿å¿«é€ŸåŠ è½½ï¼Œä½†è¿™è¯æ˜äº†ä½ æ‡‚è¿™ä¸ª pipeline
        self.model_id = "openai/clip-vit-base-patch32"
        self.model = CLIPModel.from_pretrained(self.model_id)
        self.processor = CLIPProcessor.from_pretrained(self.model_id)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        print(f"âœ… [VisualAligner] Model loaded on {self.device}")

    def get_text_embedding(self, text: str):
        """
        å°†æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºé«˜ç»´å‘é‡ (Embedding)ã€‚
        åœ¨ Thesis ä¸­ï¼Œè¿™é‡Œå¤„ç†çš„æ˜¯ 'Site Evidence Description' (ç°åœºè¯æ®æè¿°)
        æˆ–è€… BIM å…ƒç´ çš„ 'Visual Properties' (è§†è§‰å±æ€§)ã€‚
        """
        inputs = self.processor(text=[text], return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            text_features = self.model.get_text_features(**inputs)
        
        # å½’ä¸€åŒ–ï¼Œä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        return text_features / text_features.norm(dim=-1, keepdim=True)

    def find_best_match(self, query_text: str, candidate_descriptions: list):
        """
        æ ¸å¿ƒç®—æ³•ï¼šè®¡ç®— Query å‘é‡ä¸æ‰€æœ‰ Candidate å‘é‡çš„ Cosine Similarityã€‚
        æ¨¡æ‹Ÿï¼šç”¨æˆ·æè¿° vs. BIM å…ƒç´ çš„è§†è§‰ç‰¹å¾æè¿°ã€‚
        """
        print(f"ğŸ” [VisualAligner] Computing Vector Similarity for: '{query_text}'")
        
        query_emb = self.get_text_embedding(query_text)
        
        scores = []
        for candidate in candidate_descriptions:
            cand_emb = self.get_text_embedding(candidate)
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)
            score = (query_emb @ cand_emb.T).item()
            scores.append(score)
            
        # æ‰¾åˆ°åŒ¹é…åº¦æœ€é«˜çš„ç´¢å¼•
        best_idx = np.argmax(scores)
        return best_idx, scores[best_idx], candidate_descriptions[best_idx]

# å•å…ƒæµ‹è¯• (é¢è¯•æ—¶å¯ä»¥è¯´ä½ å†™è¿‡å•å…ƒæµ‹è¯•æ¥éªŒè¯å‘é‡å¯¹é½)
if __name__ == "__main__":
    aligner = VisualAligner()
    site_observation = "Cracked grey concrete surface"
    bim_elements = [
        "Wooden kitchen cabinet", 
        "Grey concrete structural slab", 
        "White painted drywall"
    ]
    idx, score, match = aligner.find_best_match(site_observation, bim_elements)
    print(f"Input: {site_observation}")
    print(f"Match: {match} (Score: {score:.4f})")